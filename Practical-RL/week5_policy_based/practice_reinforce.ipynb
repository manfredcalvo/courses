{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSZCfFb-8tRC"
   },
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnvwLfy08tRI",
    "outputId": "c6803564-efe3-441f-abff-bb1d4c47105b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Selecting previously unselected package xvfb.\n",
      "(Reading database ... 145483 files and directories currently installed.)\n",
      "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
      "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
      "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Starting virtual X frame buffer: Xvfb.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
    "\n",
    "        !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O6_elCMg8tRL"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnnQpDry8tRL"
   },
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "9BtYi4pW8tRL",
    "outputId": "acd7a7b4-ee54-4837-a54f-49b94ae24648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fccb0f9c6a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSjqB1fr8tRM"
   },
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3kj_dNc8tRM"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APQT38kP8tRM",
    "outputId": "d9ea9d8b-b8ee-4aaa-c31e-3b86467e58eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "kl6aZF9r8tRN"
   },
   "outputs": [],
   "source": [
    "# create input variables. We only need <s, a, r> for REINFORCE\n",
    "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
    "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "n_Bxk23c8tRN"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=state_dim))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(n_actions))\n",
    "\n",
    "logits = model(ph_states)\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "lsZfOftU8tRN"
   },
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "PL6qcI8S8tRO"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    return policy.eval({ph_states: [states]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI7i5-Jz8tRP"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "7DwSP9Vc8tRP"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(s)\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "r7kIwUAl8tRQ"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQLmAorW8tRQ"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "s_em7YFx8tRQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    discounted_reward = 0\n",
    "\n",
    "    n_rewards = len(rewards)\n",
    "\n",
    "    discount_rewards = np.zeros(n_rewards)\n",
    "\n",
    "    for i in range(n_rewards - 1, -1, -1):\n",
    "      if i == n_rewards - 1:\n",
    "        discount_rewards[i] = rewards[i]\n",
    "      else:\n",
    "        discount_rewards[i] = rewards[i] + gamma * discount_rewards[i + 1]\n",
    "        \n",
    "    return list(discount_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aj7qaWxP8tRQ",
    "outputId": "bf295bd5-1fe5-48f3-bc87-14d627279b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztiT4jKG8tRR"
   },
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "0R2ZvewN8tRR"
   },
   "outputs": [],
   "source": [
    "# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "iD5NKcpA8tRS"
   },
   "outputs": [],
   "source": [
    "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
    "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
    "# Also recall that we defined ph_cumulative_rewards earlier.\n",
    "\n",
    "J = tf.reduce_mean(ph_cumulative_rewards * log_policy_for_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etfNCGVZ8tRS"
   },
   "source": [
    "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
    "\n",
    "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3QjBChdR8tRS"
   },
   "outputs": [],
   "source": [
    "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
    "# being deterministic, harming exploration.\n",
    "\n",
    "entropy = -tf.reduce_sum(policy * tf.log(policy), 1, name=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "B4jXxLnT8tRS"
   },
   "outputs": [],
   "source": [
    "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
    "loss = -(J + 0.1 * entropy)\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "OUE_9u3F8tRT"
   },
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, t_max=1000):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    update.run({\n",
    "        ph_states: states,\n",
    "        ph_actions: actions,\n",
    "        ph_cumulative_rewards: cumulative_rewards,\n",
    "    })\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "15rPN7CN8tRU"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcyw9vkW8tRU"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeQP_mYf8tRV",
    "outputId": "75904d3f-6aa1-458b-af7a-c33696946d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 42.240\n",
      "mean reward: 140.800\n",
      "mean reward: 206.890\n",
      "mean reward: 341.400\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJbQg_w58tRV"
   },
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "cyzOJ2fA8tRW"
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501,
     "resources": {
      "http://localhost:8080/videos/openaigym.video.1.61.video000064.mp4": {
       "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
       "headers": [
        [
         "content-length",
         "1449"
        ],
        [
         "content-type",
         "text/html; charset=utf-8"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": "Not Found"
      }
     }
    },
    "id": "uYQMnWjX8tRW",
    "outputId": "50c9bd9a-ed71-4c49-b612-604e87ef6a1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.1.61.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PRfvpkz8tRX",
    "outputId": "267af70e-ffd9-4c60-8197-eb2321af8d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average reward is 418.5 over 100 episodes\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, 'calvomanfred@gmail.com', 'IkuzaZjenrCillte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL2-t7UT8tRX"
   },
   "source": [
    "That's all, thank you for your attention!\n",
    "\n",
    "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "practice_reinforce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
